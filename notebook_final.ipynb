{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text Classification Competition: Sarcasm Detection\n",
    "\n",
    "Chua Yeow Long, ylchua2@illinois.edu\n",
    "\n",
    "In this notebook, i'll first try using a simple word count model using sklearn's CountVectorizer to perform sarcasm detection. Next, I'll use GLoVe embedding and add neural network layers towards the end, training just the added layers using the provided dataset. Finally, I'll fine tune BERT layers, just fine-tuning the entire BERT model which consists of hundreds of millions of parameters using the training dataset with a pretty beefy machine."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first need to import the necessary libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.util import ngrams\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from collections import defaultdict\n",
    "from collections import  Counter\n",
    "plt.style.use('ggplot')\n",
    "stop=set(stopwords.words('english'))\n",
    "import re\n",
    "from nltk.tokenize import word_tokenize\n",
    "import gensim\n",
    "import string\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from tqdm import tqdm\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Embedding,LSTM,Dense,SpatialDropout1D\n",
    "from keras.initializers import Constant\n",
    "from sklearn.model_selection import train_test_split\n",
    "from keras.optimizers import Adam"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make use of pandas' read_json module to load the data into a pandas dataframe where we'll perform our work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_json('data/train.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>label</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER @USER I don't get this .. obviousl...</td>\n",
       "      <td>[A minor child deserves privacy and should be ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER trying to protest about . Talking ...</td>\n",
       "      <td>[@USER @USER Why is he a loser ? He's just a P...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER @USER He makes an insane about of ...</td>\n",
       "      <td>[Donald J . Trump is guilty as charged . The e...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER Meanwhile Trump won't even release...</td>\n",
       "      <td>[Jamie Raskin tanked Doug Collins . Collins lo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SARCASM</td>\n",
       "      <td>@USER @USER Pretty Sure the Anti-Lincoln Crowd...</td>\n",
       "      <td>[Man ... y ’ all gone “ both sides ” the apoca...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     label                                           response  \\\n",
       "0  SARCASM  @USER @USER @USER I don't get this .. obviousl...   \n",
       "1  SARCASM  @USER @USER trying to protest about . Talking ...   \n",
       "2  SARCASM  @USER @USER @USER He makes an insane about of ...   \n",
       "3  SARCASM  @USER @USER Meanwhile Trump won't even release...   \n",
       "4  SARCASM  @USER @USER Pretty Sure the Anti-Lincoln Crowd...   \n",
       "\n",
       "                                             context  \n",
       "0  [A minor child deserves privacy and should be ...  \n",
       "1  [@USER @USER Why is he a loser ? He's just a P...  \n",
       "2  [Donald J . Trump is guilty as charged . The e...  \n",
       "3  [Jamie Raskin tanked Doug Collins . Collins lo...  \n",
       "4  [Man ... y ’ all gone “ both sides ” the apoca...  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "## Data-Preprocessing\n",
    "\n",
    "We'll just do a bit of preprocessing by removing non-alphabets and removing stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refineWords(s):\n",
    "\n",
    "    letters_only = re.sub(\"[^a-zA-Z]\", \" \", str(s))\n",
    "    words = letters_only.lower().split()\n",
    "    stops = set(stopwords.words(\"english\"))\n",
    "    meaningful_words = [w for w in words if not w in stops]\n",
    "\n",
    "    return( \" \".join( meaningful_words ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With or without context\n",
    "\n",
    "We'll just combine the context into the response. I suppose having extra information/context is always good."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['response'] = df['response'].apply(refineWords)\n",
    "df['context'] = df['context'].apply(refineWords)\n",
    "\n",
    "df['response'] = df['context'] + ' ' + df['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to change the predictor variable which consists of 'SARCASM' and 'NON_SARCASM' to '1's and '0's so that we can feed them into the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarcasm_mapping(input):\n",
    "    if input == 'SARCASM':\n",
    "        return 1\n",
    "    else:\n",
    "        return 0\n",
    "\n",
    "df['label'] = df['label'].apply(sarcasm_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We initialize a simple countvectorizer from sklearn for our first model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "vectorizer = CountVectorizer(analyzer = \"word\",   \\\n",
    "                             tokenizer = None,    \\\n",
    "                             preprocessor = None, \\\n",
    "                             stop_words = None,   \\\n",
    "                             max_features = 5000) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We duplicated copies of the training dataset so we have individual copies to work on for each model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_tmp = df.copy()\n",
    "df_tmp_bert = df.copy()\n",
    "df_tmp_xlm = df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Preprocessing\n",
    "\n",
    "We'll need to ensure our training data is in the correct format for further analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[\"response\"] = vectorizer.fit_transform(df[\"response\"]).toarray()\n",
    "df[\"context\"] = vectorizer.fit_transform(df[\"context\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling\n",
    "\n",
    "We'll train our first model which is a simple word count model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "forest = RandomForestClassifier(n_estimators = 100)\n",
    "features_forest = df[[\"response\",\"context\"]].values\n",
    "my_forest = forest.fit(features_forest, df['label'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We check the distribution of the target variable to see and indeed, the dataset is perfectly balanced."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1    2500\n",
       "0    2500\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data-Preprocessing for test set\n",
    "\n",
    "We do a similar preprocessing for the test set as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_json('data/test.jsonl', lines=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>twitter_1</td>\n",
       "      <td>@USER @USER @USER My 3 year old , that just fi...</td>\n",
       "      <td>[Well now that ’ s problematic AF &lt;URL&gt;, @USER...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>twitter_2</td>\n",
       "      <td>@USER @USER How many verifiable lies has he to...</td>\n",
       "      <td>[Last week the Fake News said that a section o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>twitter_3</td>\n",
       "      <td>@USER @USER @USER Maybe Docs just a scrub of a...</td>\n",
       "      <td>[@USER Let ’ s Aplaud Brett When he deserves i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>twitter_4</td>\n",
       "      <td>@USER @USER is just a cover up for the real ha...</td>\n",
       "      <td>[Women generally hate this president . What's ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>twitter_5</td>\n",
       "      <td>@USER @USER @USER The irony being that he even...</td>\n",
       "      <td>[Dear media Remoaners , you excitedly sharing ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          id                                           response  \\\n",
       "0  twitter_1  @USER @USER @USER My 3 year old , that just fi...   \n",
       "1  twitter_2  @USER @USER How many verifiable lies has he to...   \n",
       "2  twitter_3  @USER @USER @USER Maybe Docs just a scrub of a...   \n",
       "3  twitter_4  @USER @USER is just a cover up for the real ha...   \n",
       "4  twitter_5  @USER @USER @USER The irony being that he even...   \n",
       "\n",
       "                                             context  \n",
       "0  [Well now that ’ s problematic AF <URL>, @USER...  \n",
       "1  [Last week the Fake News said that a section o...  \n",
       "2  [@USER Let ’ s Aplaud Brett When he deserves i...  \n",
       "3  [Women generally hate this president . What's ...  \n",
       "4  [Dear media Remoaners , you excitedly sharing ...  "
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We simply combine the context into the response."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['response'] = test_df['response'].apply(refineWords)\n",
    "test_df['context'] = test_df['context'].apply(refineWords)\n",
    "\n",
    "test_df['response'] = test_df['context'] + ' ' + test_df['response']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We duplicate the test dataset so that we have individual copies to work on"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df_tmp = test_df.copy()\n",
    "test_df_tmp_bert = test_df.copy()\n",
    "test_df_tmp_xlm = test_df.copy()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to make sure that the test data is in the same format as the train data so we do essentially the same analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df[\"response\"] = vectorizer.fit_transform(test_df[\"response\"]).toarray()\n",
    "test_df[\"context\"] = vectorizer.fit_transform(test_df[\"context\"]).toarray()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input format needs to be in a form of an array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_forest_test = test_df[[\"response\",\"context\"]].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform predictions of the test features using the trained CountVectorizer model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_prediction = my_forest.predict(features_forest_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert the predictions into a pandas dataframe so we can use to_csv to easily output our predictions in the right format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = pd.DataFrame(my_prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We do a value_count on the predictions to check how well the model worked. Well the model predicted 100% no sarcasm which is clearly problematic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    1800\n",
       "Name: label, dtype: int64"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df['label'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>response</th>\n",
       "      <th>context</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.000000</td>\n",
       "      <td>1800.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.002222</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.097170</td>\n",
       "      <td>0.074523</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>4.000000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          response      context   label\n",
       "count  1800.000000  1800.000000  1800.0\n",
       "mean      0.002778     0.002222     0.0\n",
       "std       0.097170     0.074523     0.0\n",
       "min       0.000000     0.000000     0.0\n",
       "25%       0.000000     0.000000     0.0\n",
       "50%       0.000000     0.000000     0.0\n",
       "75%       0.000000     0.000000     0.0\n",
       "max       4.000000     3.000000     0.0"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After we are done making predictions using the trained model we'll need to change the 1's and 0's of the label column back to sarcasm and non-sarcasm."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarcasm_reverse_mapping(input):\n",
    "    if input == 1:\n",
    "        return 'SARCASM'\n",
    "    else:\n",
    "        return 'NOT_SARCASM'\n",
    "\n",
    "test_df['label'] = test_df['label'].apply(sarcasm_reverse_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We make use of to_csv of the pandas library to create our answer.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df[['id','label']].to_csv('answer.txt', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Modelling using GLoVe embedding and some neural network layers\n",
    "\n",
    "First, we'll need to create the corpus. We'll need the tqdm module and NLTK's word tokenize as well as stopwords to preprocess our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_corpus(df):\n",
    "    corpus=[]\n",
    "    for tweet in tqdm(df['response']):\n",
    "        words=[word.lower() for word in word_tokenize(tweet) if((word.isalpha()==1) & (word not in stop))]\n",
    "        corpus.append(words)\n",
    "    return corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We concat/combine the training and test dataset so create a corpus of both the train and test datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 6800/6800 [00:01<00:00, 3772.90it/s]\n"
     ]
    }
   ],
   "source": [
    "df_tmp['response'] = df_tmp['response'].astype('string')\n",
    "test_df_tmp['response'] = test_df_tmp['response'].astype('string')\n",
    "\n",
    "df_new = df_tmp.append(test_df_tmp)\n",
    "\n",
    "corpus=create_corpus(df_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll first need to download the glove embedding and load it ensuring the correct formats,"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dict={}\n",
    "with open('data/glove.6B.200d.txt','r', encoding=\"utf8\") as f:\n",
    "    for line in f:\n",
    "        values=line.split()\n",
    "        word=values[0]\n",
    "        vectors=np.asarray(values[1:],'float32')\n",
    "        embedding_dict[word]=vectors\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first start by initializing a Keras tokenizer and train it with the corpus we obtained earlier. We perform truncating and padding to get sequences of the same length."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LEN=50\n",
    "tokenizer_obj=Tokenizer()\n",
    "tokenizer_obj.fit_on_texts(corpus)\n",
    "sequences=tokenizer_obj.texts_to_sequences(corpus)\n",
    "\n",
    "tweet_pad=pad_sequences(sequences,maxlen=MAX_LEN,truncating='post',padding='post')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our tokenizer has a word number count in the form of word_index. We'll need it later."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_index=tokenizer_obj.word_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to make sure of tqdm module and GLoVe embedding to obtain the embedding matrix to be used to create an embedding layer using Keras."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████| 34445/34445 [00:00<00:00, 569822.52it/s]\n"
     ]
    }
   ],
   "source": [
    "num_words=len(word_index)+1\n",
    "embedding_matrix=np.zeros((num_words,200))\n",
    "\n",
    "for word,i in tqdm(word_index.items()):\n",
    "    if i > num_words:\n",
    "        continue\n",
    "    \n",
    "    emb_vec=embedding_dict.get(word)\n",
    "    if emb_vec is not None:\n",
    "        embedding_matrix[i]=emb_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll build our neural network sequentially. We first add the embedding layer and add LSTM layers of decreasing nodes with dropout to reduce overfitting."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=Sequential()\n",
    "\n",
    "embedding=Embedding(num_words,200,embeddings_initializer=Constant(embedding_matrix),\n",
    "                   input_length=MAX_LEN,trainable=False)\n",
    "\n",
    "model.add(embedding)\n",
    "model.add(SpatialDropout1D(0.10))\n",
    "model.add(LSTM(128*2, dropout=0.10, recurrent_dropout=0.10, return_sequences=True))\n",
    "model.add(Dense(64*2, activation='relu'))\n",
    "model.add(LSTM(128, dropout=0.10, recurrent_dropout=0.10, return_sequences=True))\n",
    "model.add(Dense(64, activation='relu'))\n",
    "model.add(LSTM(64, dropout=0.10, recurrent_dropout=0.10, return_sequences=True))\n",
    "model.add(Dense(32, activation='relu'))\n",
    "model.add(LSTM(32, dropout=0.10, recurrent_dropout=0.10, return_sequences=True))\n",
    "model.add(Dense(16, activation='relu'))\n",
    "model.add(LSTM(16, dropout=0.10, recurrent_dropout=0.10))\n",
    "\n",
    "model.add(Dense(1, activation='sigmoid'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll make use of the Adam optimizer with a small learning rate. We'll compile the model specifying the loss, optimizer and metrics to optimize our model for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimzer=Adam(learning_rate=1e-5*10)\n",
    "\n",
    "model.compile(loss='binary_crossentropy',optimizer=optimzer,metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We split our data back into train and test datasets. The first 5000 is our training data the rest is test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "train=tweet_pad[:5000]\n",
    "test=tweet_pad[5000:]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll perform a train-validation split to obtain validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train,X_test,y_train,y_test=train_test_split(train,df_tmp['label'].values,test_size=0.15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now start training our model specifying the batch size, epochs and validation datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4250 samples, validate on 750 samples\n",
      "Epoch 1/1\n",
      " - 11s - loss: 0.6914 - accuracy: 0.5214 - val_loss: 0.6810 - val_accuracy: 0.6293\n"
     ]
    }
   ],
   "source": [
    "history=model.fit(X_train,y_train,batch_size=4*16,epochs=1,validation_data=(X_test,y_test),verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform predictions using our trained model. As the predictions are not strictly 1's and 0's we'll just simply round them to the nearest integer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pre=model.predict(test)\n",
    "\n",
    "test_df['label'] = np.round(y_pre).astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to do a mapping of 0's and 1's to 'NOT_SARCASM' and 'SARCASM'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sarcasm_reverse_mapping(input):\n",
    "    if input == 1:\n",
    "        return 'SARCASM'\n",
    "    else:\n",
    "        return 'NOT_SARCASM'\n",
    "\n",
    "test_df['label'] = test_df['label'].apply(sarcasm_reverse_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We convert our predictions into a csv file with the right format with the following."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df[['id','label']].to_csv('answer.txt', index=False, header=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Modelling using BERT, retraining the entire BERT architecture to the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use the official tokenization script from tensorflow. You can download the tokenization.py by using wget or downloading it manually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "SYSTEM_WGETRC = c:/progra~1/wget/etc/wgetrc\n",
      "syswgetrc = C:\\Program Files (x86)\\GnuWin32/etc/wgetrc\n"
     ]
    }
   ],
   "source": [
    "!wget --quiet https://raw.githubusercontent.com/tensorflow/models/master/official/nlp/bert/tokenization.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll import the necessary libraries needed for this section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import ModelCheckpoint\n",
    "import tensorflow_hub as hub\n",
    "\n",
    "import tokenization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to encode the input texts to BERT's input format. For each row of the training data, we first tokenize the text using NLTK tokenizer before proceeding to convert the obtained tokens into IDs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bert_encode(texts, tokenizer, max_len=512):\n",
    "    all_tokens = []\n",
    "    all_masks = []\n",
    "    all_segments = []\n",
    "    \n",
    "    for text in texts:\n",
    "        text = tokenizer.tokenize(text)\n",
    "            \n",
    "        text = text[:max_len-2]\n",
    "        input_sequence = [\"[CLS]\"] + text + [\"[SEP]\"]\n",
    "        pad_len = max_len - len(input_sequence)\n",
    "        \n",
    "        tokens = tokenizer.convert_tokens_to_ids(input_sequence)\n",
    "        tokens += [0] * pad_len\n",
    "        pad_masks = [1] * len(input_sequence) + [0] * pad_len\n",
    "        segment_ids = [0] * max_len\n",
    "        \n",
    "        all_tokens.append(tokens)\n",
    "        all_masks.append(pad_masks)\n",
    "        all_segments.append(segment_ids)\n",
    "    \n",
    "    return np.array(all_tokens), np.array(all_masks), np.array(all_segments)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll create the BERT layer specifying the inputs and creation of the BERT layer using the inputs to obtain the output sequence. Since we are re-training the entire BERT architecture to the dataset, we'll just add a sigmoid dense layer to perform classification. We specify the BERT model and the adam optimizer, optimization loss and accuracy metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(bert_layer, max_len=512):\n",
    "    input_word_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_word_ids\")\n",
    "    input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"input_mask\")\n",
    "    segment_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "    _, sequence_output = bert_layer([input_word_ids, input_mask, segment_ids])\n",
    "    clf_output = sequence_output[:, 0, :]\n",
    "    \n",
    "    # Without Dropout\n",
    "    out = Dense(1, activation='sigmoid')(clf_output)\n",
    "\n",
    "    model = Model(inputs=[input_word_ids, input_mask, segment_ids], outputs=out)\n",
    "    model.compile(Adam(lr=learning_rate), loss='binary_crossentropy', metrics=['accuracy'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We load BERT from TensorFlow Hub. TensorFlow Hub provides pre-trained models for us to use and we load the model using TensorFlow hub module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "module_url = \"https://tfhub.dev/tensorflow/bert_en_uncased_L-24_H-1024_A-16/1\"\n",
    "\n",
    "bert_layer = hub.KerasLayer(module_url, trainable=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to load the tokenizer from the BERT layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_file = bert_layer.resolved_object.vocab_file.asset_path.numpy()\n",
    "do_lower_case = bert_layer.resolved_object.do_lower_case.numpy()\n",
    "tokenizer = tokenization.FullTokenizer(vocab_file, do_lower_case)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we process the text into BERT required formats such as tokens, masks and segment flags."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_input = bert_encode(df_tmp_bert.response.values, tokenizer, max_len=160)\n",
    "test_input = bert_encode(test_df_tmp_bert.response.values, tokenizer, max_len=160)\n",
    "train_labels = df_tmp_bert.label.values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This are the parameters we'll need for our BERT model. Due to limited processing capacity, there aint much room for me to play with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "Max_length = 42\n",
    "Dropout_num = 0  \n",
    "learning_rate = 6e-6 \n",
    "valid = 0.15 \n",
    "epochs_num = 5 \n",
    "batch_size_num = 4 \n",
    "ids_error_corrected = True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We build our BERT model and note that we have 335 million traininable parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_word_ids (InputLayer)     [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_mask (InputLayer)         [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "segment_ids (InputLayer)        [(None, 160)]        0                                            \n",
      "__________________________________________________________________________________________________\n",
      "keras_layer (KerasLayer)        [(None, 1024), (None 335141889   input_word_ids[0][0]             \n",
      "                                                                 input_mask[0][0]                 \n",
      "                                                                 segment_ids[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "tf_op_layer_strided_slice (Tens [(None, 1024)]       0           keras_layer[0][1]                \n",
      "__________________________________________________________________________________________________\n",
      "dense (Dense)                   (None, 1)            1025        tf_op_layer_strided_slice[0][0]  \n",
      "==================================================================================================\n",
      "Total params: 335,142,914\n",
      "Trainable params: 335,142,913\n",
      "Non-trainable params: 1\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_BERT = build_model(bert_layer, max_len=160)\n",
    "model_BERT.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll train the BERT model, serializing the best model to a file for later predictions. Finally, we get to train our BERT model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 4250 samples, validate on 750 samples\n",
      "Epoch 1/5\n",
      "4250/4250 [==============================] - 303s 71ms/sample - loss: 0.5174 - accuracy: 0.7588 - val_loss: 0.5598 - val_accuracy: 0.6080\n",
      "Epoch 2/5\n",
      "4250/4250 [==============================] - 278s 65ms/sample - loss: 0.4388 - accuracy: 0.8024 - val_loss: 0.4221 - val_accuracy: 0.7653\n",
      "Epoch 3/5\n",
      "4250/4250 [==============================] - 265s 62ms/sample - loss: 0.3094 - accuracy: 0.8727 - val_loss: 1.3605 - val_accuracy: 0.4267\n",
      "Epoch 4/5\n",
      "4250/4250 [==============================] - 265s 62ms/sample - loss: 0.1140 - accuracy: 0.9614 - val_loss: 2.7396 - val_accuracy: 0.3747\n",
      "Epoch 5/5\n",
      "4250/4250 [==============================] - 265s 62ms/sample - loss: 0.0422 - accuracy: 0.9861 - val_loss: 2.8826 - val_accuracy: 0.4853\n"
     ]
    }
   ],
   "source": [
    "checkpoint = ModelCheckpoint('model_BERT.h5', monitor='val_loss', save_best_only=True)\n",
    "\n",
    "train_history = model_BERT.fit(\n",
    "    train_input, train_labels,\n",
    "    validation_split = valid,\n",
    "    epochs = epochs_num, \n",
    "    callbacks=[checkpoint],\n",
    "    batch_size = batch_size_num\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We perform predictions by loading weights from the file we serialized the model earlier. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_BERT.load_weights('model_BERT.h5')\n",
    "test_pred_BERT = model_BERT.predict(test_input)\n",
    "test_pred_BERT_int = test_pred_BERT.round().astype('int')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to perform a mapping of the 1's and 0's back to 'SARCASM' and 'NOT_SARCASM' for submission. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df['label'] = test_pred_BERT_int\n",
    "def sarcasm_reverse_mapping(input):\n",
    "    if input == 1:\n",
    "        return 'SARCASM'\n",
    "    else:\n",
    "        return 'NOT_SARCASM'\n",
    "\n",
    "test_df['label'] = test_df['label'].apply(sarcasm_reverse_mapping)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We generate the 'answer.txt' in the format required for submission."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test_df[['id','label']].to_csv('answer.txt', index=False, header=None)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
